{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f19fe0b3d5fc4af9a0bd83edad98aad9",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# Wikidata: The state of the art (IMAGES)\n",
    "\n",
    "This notebook consolidates all Wikidata processing for VKUS Viewer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "483906129a284b5d81866eb79c0262a4",
    "deepnote_cell_type": "code",
    "execution_context_id": "7d918ffb-41c0-4b94-8230-fbff3e641a5a",
    "execution_millis": 0,
    "execution_start": 1744742297734,
    "source_hash": "655c3010"
   },
   "outputs": [],
   "source": [
    "!pip3 install SPARQLWrapper\n",
    "!pip install pyvips pillow requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DYLD_LIBRARY_PATH\"] = \"/opt/homebrew/lib\"\n",
    "\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import pyvips\n",
    "import tempfile\n",
    "import shutil\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "ENDPOINT_URL = \"https://query.wikidata.org/sparql\"\n",
    "USER_AGENT = \"Data on Paintings; Marian Dörk\"\n",
    "SAFE_COMMA = \"，\"  # fullwidth comma\n",
    "\n",
    "CACHE_DIR = \"wikidata_cache\"\n",
    "IMAGES_DIR = \"vv/data/images\"\n",
    "SKIPPED_DIR = \"vv/data/images/skipped\"\n",
    "\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(SKIPPED_DIR, exist_ok=True)\n",
    "\n",
    "ENTITY_URL_TEMPLATE = \"https://www.wikidata.org/wiki/Special:EntityData/{}.json\"\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def ts():\n",
    "    return datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "def query_hash(query_template: str) -> str:\n",
    "    \"\"\"Stable hash for a query string.\"\"\"\n",
    "    h = hashlib.sha1()\n",
    "    h.update(query_template.encode(\"utf-8\"))\n",
    "    return h.hexdigest()[:12]\n",
    "\n",
    "def get_sparql(query, max_retries=3):\n",
    "    \"\"\"Execute SPARQL query with caching and retry logic.\"\"\"\n",
    "    qhash = query_hash(query)\n",
    "    cache_file = os.path.join(CACHE_DIR, f\"{qhash}.json\")\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"r\") as f:\n",
    "            print(f\"{ts()} | [cache] {qhash}\")\n",
    "            return json.load(f)\n",
    "    \n",
    "    sparql = SPARQLWrapper(ENDPOINT_URL, agent=USER_AGENT)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"{ts()} | [query] {qhash}...\")\n",
    "            \n",
    "            # Get raw response and clean control characters before parsing\n",
    "            response = sparql.query()\n",
    "            raw_data = response.response.read().decode('utf-8')\n",
    "            cleaned_data = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', ' ', raw_data)\n",
    "            result = json.loads(cleaned_data)\n",
    "            \n",
    "            with open(cache_file, \"w\") as f:\n",
    "                json.dump(result, f)\n",
    "            \n",
    "            print(f\"{ts()} | [saved] {qhash}\")\n",
    "            time.sleep(2)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait = (attempt + 1) * 5\n",
    "                print(f\"{ts()} | [retry] Attempt {attempt + 1} failed: {e}\")\n",
    "                print(f\"{ts()} | [retry] Waiting {wait}s...\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "def get_entity(qid, session=None):\n",
    "    \"\"\"Get entity data from cache or fetch from Wikidata.\"\"\"\n",
    "    cache_file = os.path.join(CACHE_DIR, f\"{qid}.json\")\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "        session.headers.update({\"User-Agent\": USER_AGENT})\n",
    "    \n",
    "    url = ENTITY_URL_TEMPLATE.format(qid)\n",
    "    resp = session.get(url, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    entity = data.get(\"entities\", {}).get(qid)\n",
    "    \n",
    "    if entity:\n",
    "        with open(cache_file, \"w\") as f:\n",
    "            json.dump(entity, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return entity\n",
    "\n",
    "def get_label(qid, lang=\"en\", session=None):\n",
    "    \"\"\"Get label for a Q-ID from cached entity data.\"\"\"\n",
    "    entity = get_entity(qid, session)\n",
    "    if entity is None:\n",
    "        return qid\n",
    "    \n",
    "    labels = entity.get(\"labels\", {})\n",
    "    if lang in labels:\n",
    "        return labels[lang][\"value\"]\n",
    "    elif \"en\" in labels:\n",
    "        return labels[\"en\"][\"value\"]\n",
    "    elif labels:\n",
    "        return list(labels.values())[0][\"value\"]\n",
    "    return qid\n",
    "\n",
    "def get_claim_values(entity, property_id):\n",
    "    \"\"\"Extract Q-IDs from a property's claims.\"\"\"\n",
    "    if entity is None:\n",
    "        return []\n",
    "    \n",
    "    claims = entity.get(\"claims\", {}).get(property_id, [])\n",
    "    values = []\n",
    "    for claim in claims:\n",
    "        mainsnak = claim.get(\"mainsnak\", {})\n",
    "        datavalue = mainsnak.get(\"datavalue\", {})\n",
    "        if datavalue.get(\"type\") == \"wikibase-entityid\":\n",
    "            values.append(datavalue[\"value\"][\"id\"])\n",
    "    return values\n",
    "\n",
    "def format_time_remaining(seconds):\n",
    "    \"\"\"Format seconds into human-readable string.\"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.0f}s\"\n",
    "    elif seconds < 3600:\n",
    "        return f\"{seconds / 60:.0f}m\"\n",
    "    else:\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        return f\"{hours}h {minutes}m\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. INITIAL WIKIDATA QUERY FOR IDS\n",
    "# =============================================================================\n",
    "\n",
    "painting_qids_query = \"\"\"\n",
    "SELECT DISTINCT ?painting WHERE {\n",
    "  ?painting wdt:P31/wdt:P279* wd:Q3305213;\n",
    "            wdt:P18 ?image;\n",
    "            wdt:P571 ?inception.\n",
    "  ?painting wikibase:sitelinks ?sitelinks.\n",
    "  FILTER(?sitelinks >= 3)\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# painting_qids_query = \"\"\"\n",
    "# SELECT DISTINCT ?painting WHERE {\n",
    "#   ?painting wdt:P31/wdt:P279* wd:Q3305213 ;\n",
    "#             wdt:P18 ?image ;\n",
    "#             wdt:P571 ?inception .\n",
    "#   ?painting wikibase:sitelinks ?paintingSitelinks .\n",
    "#   OPTIONAL {\n",
    "#     ?painting wdt:P170 ?creator .\n",
    "#     ?creator wikibase:sitelinks ?creatorSitelinks .\n",
    "#   }\n",
    "#   FILTER(?paintingSitelinks >= 10 || ?creatorSitelinks >= 100)\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "print(f\"{ts()} | Querying Wikidata for paintings...\")\n",
    "\n",
    "result = get_sparql(painting_qids_query)\n",
    "painting_rows = result[\"results\"][\"bindings\"]\n",
    "painting_qids = [row[\"painting\"][\"value\"].split(\"/\")[-1] for row in painting_rows]\n",
    "\n",
    "print(f\"{ts()} | Retrieved {len(painting_qids):,} unique paintings\")\n",
    "print(f\"{ts()} | Criteria: has image, has inception date, 3+ sitelinks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. FETCH WIKIDATA ENTITY JSONs per Q-ID\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nFetching entity data for {len(painting_qids):,} paintings...\")\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": USER_AGENT})\n",
    "\n",
    "# Adaptive retry timing\n",
    "retry_sleep = 10.0\n",
    "min_retry_sleep = 10.0\n",
    "max_retry_sleep = 3600.0\n",
    "consecutive_successes = 0\n",
    "\n",
    "# Progress tracking\n",
    "total = len(painting_qids)\n",
    "fetched = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for i, qid in enumerate(painting_qids):\n",
    "    cache_file = os.path.join(CACHE_DIR, f\"{qid}.json\")\n",
    "    \n",
    "    # Already cached\n",
    "    if os.path.exists(cache_file):\n",
    "        consecutive_successes += 1\n",
    "        if consecutive_successes >= 2:\n",
    "            retry_sleep = max(retry_sleep / 2, min_retry_sleep)\n",
    "            consecutive_successes = 0\n",
    "        continue\n",
    "    \n",
    "    fetched += 1\n",
    "    \n",
    "    # Progress with time estimate\n",
    "    if fetched % 50 == 0 or fetched == 1:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = fetched / elapsed if elapsed > 0 else 0\n",
    "        \n",
    "        remaining_to_check = total - (i + 1)\n",
    "        checked_so_far = i + 1\n",
    "        cache_hit_rate = 1 - (fetched / checked_so_far) if checked_so_far > 0 else 0\n",
    "        estimated_remaining_fetches = remaining_to_check * (1 - cache_hit_rate)\n",
    "        estimated_seconds = estimated_remaining_fetches / rate if rate > 0 else 0\n",
    "        \n",
    "        print(f\"{ts()} | [progress] {fetched:,} fetched @ {rate:.1f}/s | {i + 1:,}/{total:,} checked | ~{format_time_remaining(estimated_seconds)} remaining\")\n",
    "    \n",
    "    # Fetch with retry\n",
    "    url = ENTITY_URL_TEMPLATE.format(qid)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            resp = session.get(url, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            entity = data.get(\"entities\", {}).get(qid)\n",
    "            \n",
    "            if not entity:\n",
    "                print(f\"{ts()} | {qid} | empty entity\")\n",
    "                break\n",
    "            \n",
    "            with open(cache_file, \"w\") as f:\n",
    "                json.dump(entity, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            consecutive_successes += 1\n",
    "            if consecutive_successes >= 2:\n",
    "                retry_sleep = max(retry_sleep / 2, min_retry_sleep)\n",
    "                consecutive_successes = 0\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            consecutive_successes = 0\n",
    "            retry_sleep = min(retry_sleep * 2, max_retry_sleep)\n",
    "            msg = str(e).split(\"\\n\")[0][:80]\n",
    "            print(f\"{ts()} | {qid} | retry in {retry_sleep:.1f}s ({msg})\")\n",
    "            time.sleep(retry_sleep)\n",
    "\n",
    "elapsed_total = time.time() - start_time\n",
    "print(f\"\\n{ts()} | [done] Fetched {fetched:,} new entities in {format_time_remaining(elapsed_total)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. DOWNLOAD IMAGES\n",
    "# =============================================================================\n",
    "\n",
    "MAX_DIM = 5000\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": USER_AGENT})\n",
    "\n",
    "print(f\"{ts()} | Checking which images need to be downloaded...\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Pre-check: identify paintings that need downloading\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "to_download = []\n",
    "\n",
    "for qid in painting_qids:\n",
    "    out_path = os.path.join(IMAGES_DIR, f\"{qid}.jpg\")\n",
    "    skipped_path = os.path.join(SKIPPED_DIR, f\"{qid}.jpg\")\n",
    "    \n",
    "    # Already exists in images/\n",
    "    if os.path.exists(out_path) and os.path.getsize(out_path) > 0:\n",
    "        continue\n",
    "    \n",
    "    # Exists in skipped/ - will be restored\n",
    "    if os.path.exists(skipped_path) and os.path.getsize(skipped_path) > 0:\n",
    "        shutil.move(skipped_path, out_path)\n",
    "        continue\n",
    "    \n",
    "    to_download.append(qid)\n",
    "\n",
    "print(f\"{ts()} | Total paintings: {len(painting_qids):,}\")\n",
    "print(f\"{ts()} | Already downloaded: {len(painting_qids) - len(to_download):,}\")\n",
    "print(f\"{ts()} | Remaining to download: {len(to_download):,}\")\n",
    "\n",
    "if not to_download:\n",
    "    print(f\"{ts()} | [done] All images already downloaded\")\n",
    "else:\n",
    "    # ---------------------------------------------------------------------------\n",
    "    # Adaptive rate limiting\n",
    "    # ---------------------------------------------------------------------------\n",
    "    SLEEP_STEPS = [1, 5, 15, 30, 60, 120, 300, 600]\n",
    "    current_step = 0\n",
    "    success_streak = 0\n",
    "    STREAK_THRESHOLD = 100\n",
    "\n",
    "    def get_current_sleep():\n",
    "        return SLEEP_STEPS[current_step]\n",
    "\n",
    "    def wait_with_backoff():\n",
    "        \"\"\"Sleep for current delay, used between requests.\"\"\"\n",
    "        sleep_time = get_current_sleep()\n",
    "        if current_step > 0:\n",
    "            print(f\"{ts()} | ... sleeping {sleep_time}s (rate limited, step {current_step}/{len(SLEEP_STEPS)-1})\")\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    def get_image_filename(entity):\n",
    "        \"\"\"Extract image filename from P18 claim.\"\"\"\n",
    "        for c in entity.get(\"claims\", {}).get(\"P18\", []):\n",
    "            dv = c.get(\"mainsnak\", {}).get(\"datavalue\")\n",
    "            if dv and dv.get(\"type\") == \"string\":\n",
    "                filename = dv.get(\"value\", \"\")\n",
    "                if filename:\n",
    "                    return filename.replace(\" \", \"_\")\n",
    "        return None\n",
    "\n",
    "    def refetch_entity(qid):\n",
    "        \"\"\"Force re-fetch entity from Wikidata (bypass cache).\"\"\"\n",
    "        cache_file = os.path.join(CACHE_DIR, f\"{qid}.json\")\n",
    "        \n",
    "        # Remove cached version\n",
    "        if os.path.exists(cache_file):\n",
    "            os.remove(cache_file)\n",
    "        \n",
    "        # Fetch fresh\n",
    "        url = ENTITY_URL_TEMPLATE.format(qid)\n",
    "        try:\n",
    "            resp = session.get(url, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            entity = data.get(\"entities\", {}).get(qid)\n",
    "            \n",
    "            if entity:\n",
    "                with open(cache_file, \"w\") as f:\n",
    "                    json.dump(entity, f, ensure_ascii=False)\n",
    "            \n",
    "            return entity\n",
    "        except Exception as e:\n",
    "            print(f\"{ts()} | {qid} | refetch ERROR ({str(e)[:80]})\")\n",
    "            return None\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "    # Download remaining images\n",
    "    # ---------------------------------------------------------------------------\n",
    "    total = len(to_download)\n",
    "    downloaded = 0\n",
    "    errors = 0\n",
    "    no_image = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"\\n{ts()} | Starting download of {total:,} images...\")\n",
    "\n",
    "    for i, qid in enumerate(to_download):\n",
    "        out_path = os.path.join(IMAGES_DIR, f\"{qid}.jpg\")\n",
    "\n",
    "        # Load entity from cache\n",
    "        entity = get_entity(qid, session)\n",
    "        \n",
    "        if entity is None:\n",
    "            errors += 1\n",
    "            continue\n",
    "\n",
    "        # Extract image filename (P18)\n",
    "        filename = get_image_filename(entity)\n",
    "        \n",
    "        # If no filename, try re-fetching entity\n",
    "        if not filename:\n",
    "            print(f\"{ts()} | {qid} | No image filename, re-fetching entity...\")\n",
    "            entity = refetch_entity(qid)\n",
    "            if entity:\n",
    "                filename = get_image_filename(entity)\n",
    "        \n",
    "        if not filename:\n",
    "            print(f\"{ts()} | {qid} | SKIP (no valid image filename)\")\n",
    "            no_image += 1\n",
    "            continue\n",
    "\n",
    "        image_url = \"https://commons.wikimedia.org/wiki/Special:FilePath/\" + urllib.parse.quote(filename)\n",
    "\n",
    "        # Download → resize → JPEG (with retry on 429)\n",
    "        max_retries = 10\n",
    "        retry_count = 0\n",
    "        \n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                wait_with_backoff()\n",
    "                \n",
    "                with session.get(image_url, stream=True, timeout=30) as resp:\n",
    "                    resp.raise_for_status()\n",
    "\n",
    "                    with tempfile.NamedTemporaryFile(delete=False) as tmp:\n",
    "                        for chunk in resp.iter_content(8192):\n",
    "                            if chunk:\n",
    "                                tmp.write(chunk)\n",
    "                        tmp_path = tmp.name\n",
    "\n",
    "                img = pyvips.Image.new_from_file(tmp_path, access=\"sequential\")\n",
    "\n",
    "                # Downscale if image area exceeds MAX_DIM * MAX_DIM\n",
    "                max_area = MAX_DIM * MAX_DIM\n",
    "                area = img.width * img.height\n",
    "                scale = min(1.0, (max_area / area) ** 0.5)\n",
    "                if scale < 1.0:\n",
    "                    img = img.resize(scale)\n",
    "\n",
    "                img.write_to_file(out_path, Q=90, strip=True)\n",
    "                os.remove(tmp_path)\n",
    "                \n",
    "                downloaded += 1\n",
    "                \n",
    "                # Progress with time estimate\n",
    "                if downloaded % 25 == 0 or downloaded == 1:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    rate = downloaded / elapsed if elapsed > 0 else 0\n",
    "                    remaining = total - (i + 1)\n",
    "                    estimated_seconds = remaining / rate if rate > 0 else 0\n",
    "                    \n",
    "                    print(f\"{ts()} | [progress] {downloaded:,}/{total:,} downloaded @ {rate:.2f}/s | ~{format_time_remaining(estimated_seconds)} remaining\")\n",
    "                \n",
    "                # Success: track streak and step down cautiously\n",
    "                if current_step > 0:\n",
    "                    success_streak += 1\n",
    "                    if success_streak >= STREAK_THRESHOLD:\n",
    "                        current_step -= 1\n",
    "                        success_streak = 0\n",
    "                        print(f\"{ts()} | ↓ stepping down to {get_current_sleep()}s delay\")\n",
    "                \n",
    "                break  # exit retry loop on success\n",
    "\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                if e.response.status_code == 429:\n",
    "                    retry_count += 1\n",
    "                    success_streak = 0\n",
    "                    if current_step < len(SLEEP_STEPS) - 1:\n",
    "                        current_step += 1\n",
    "                    print(f\"{ts()} | {qid} | 429 rate limited, retry {retry_count}/{max_retries}, step up → {get_current_sleep()}s\")\n",
    "                    if retry_count >= max_retries:\n",
    "                        print(f\"{ts()} | {qid} | ERROR (max retries exceeded)\")\n",
    "                        errors += 1\n",
    "                elif e.response.status_code == 404:\n",
    "                    print(f\"{ts()} | {qid} | SKIP (image not found: {filename[:50]}...)\")\n",
    "                    no_image += 1\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"{ts()} | {qid} | ERROR ({str(e)[:80]})\")\n",
    "                    errors += 1\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"{ts()} | {qid} | ERROR ({str(e)[:80]})\")\n",
    "                if os.path.exists(out_path):\n",
    "                    os.remove(out_path)\n",
    "                errors += 1\n",
    "                break\n",
    "\n",
    "    elapsed_total = time.time() - start_time\n",
    "    print(f\"\\n{ts()} | [done] Downloaded {downloaded:,}, no image {no_image:,}, errors {errors:,} in {format_time_remaining(elapsed_total)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. BUILD RECORDS FROM PAINTING ENTITIES\n",
    "# =============================================================================\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Helper functions\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def normalize_value(value):\n",
    "    if not value:\n",
    "        return \"\"\n",
    "    return value.replace(\",\", SAFE_COMMA).strip()\n",
    "\n",
    "def add_keyword(rec, prefix, qid):\n",
    "    \"\"\"Add a keyword in format 'prefix:Q12345' to the record's keywords set.\"\"\"\n",
    "    if not prefix or not qid:\n",
    "        return\n",
    "    if not qid.startswith(\"Q\"):\n",
    "        return\n",
    "    rec[\"keywords\"].add(f\"{prefix}:{qid}\")\n",
    "\n",
    "def add_column_value(rec, field, qids):\n",
    "    if not qids:\n",
    "        return\n",
    "    existing = [v.strip() for v in rec[field].split(\",\") if v.strip()]\n",
    "    for q in qids:\n",
    "        if q not in existing:\n",
    "            existing.append(q)\n",
    "    rec[field] = \", \".join(existing)\n",
    "\n",
    "def parse_wikidata_time(timeval):\n",
    "    \"\"\"Convert Wikidata time string like '+1505-01-01T00:00:00Z' to int year.\"\"\"\n",
    "    if not timeval:\n",
    "        return None\n",
    "    m = re.match(r'^\\+?(\\d+)', timeval)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    return None\n",
    "\n",
    "def resolve_claim_qids(entity, prop):\n",
    "    \"\"\"Return list of QIDs for a given property (Pxxx) from claims.\"\"\"\n",
    "    out = []\n",
    "    for c in entity.get(\"claims\", {}).get(prop, []):\n",
    "        dv = c.get(\"mainsnak\", {}).get(\"datavalue\")\n",
    "        if dv and dv.get(\"type\") == \"wikibase-entityid\":\n",
    "            out.append(f\"Q{dv['value']['numeric-id']}\")\n",
    "    return out\n",
    "\n",
    "def resolve_claim_values(entity, prop):\n",
    "    \"\"\"Return list of string values (for P18, etc.) from claims.\"\"\"\n",
    "    out = []\n",
    "    for c in entity.get(\"claims\", {}).get(prop, []):\n",
    "        dv = c.get(\"mainsnak\", {}).get(\"datavalue\")\n",
    "        if not dv:\n",
    "            continue\n",
    "        if dv[\"type\"] == \"string\":\n",
    "            out.append(dv[\"value\"])\n",
    "        elif dv[\"type\"] == \"time\":\n",
    "            out.append(dv[\"value\"][\"time\"])\n",
    "    return out\n",
    "\n",
    "def clean_movement_label(label):\n",
    "    \"\"\"Clean up movement labels by removing ' painting' suffix.\"\"\"\n",
    "    if not label:\n",
    "        return label\n",
    "    if label.lower().endswith(\" painting\"):\n",
    "        label = label[:-9]\n",
    "    return label.title()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Pre-scan: Build movement QID harmonization map\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"{ts()} | Pre-scanning movements for harmonization...\")\n",
    "\n",
    "# First, collect all unique movement QIDs from paintings and artists\n",
    "all_movement_qids = set()\n",
    "\n",
    "for i, qid in enumerate(painting_qids):\n",
    "    if (i + 1) % 100000 == 0:\n",
    "        print(f\"{ts()} | [scanning paintings] {i + 1:,}/{len(painting_qids):,}\")\n",
    "    \n",
    "    entity = get_entity(qid)\n",
    "    if entity is None:\n",
    "        continue\n",
    "    \n",
    "    all_movement_qids.update(resolve_claim_qids(entity, \"P135\"))\n",
    "\n",
    "print(f\"{ts()} | Unique movement QIDs from paintings: {len(all_movement_qids):,}\")\n",
    "\n",
    "# We'll also need to scan artist movements later, but first let's fetch labels\n",
    "# for what we have so far\n",
    "\n",
    "# Fetch movement entities if not cached\n",
    "movements_to_fetch = [qid for qid in all_movement_qids if not os.path.exists(os.path.join(CACHE_DIR, f\"{qid}.json\"))]\n",
    "\n",
    "if movements_to_fetch:\n",
    "    print(f\"{ts()} | Fetching {len(movements_to_fetch):,} movement entities...\")\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\"User-Agent\": USER_AGENT})\n",
    "    \n",
    "    for i, qid in enumerate(movements_to_fetch):\n",
    "        try:\n",
    "            get_entity(qid, session)\n",
    "        except Exception as e:\n",
    "            print(f\"{ts()} | {qid} | ERROR ({str(e)[:80]})\")\n",
    "\n",
    "# Build harmonization map: movement QID -> canonical QID\n",
    "# Multiple QIDs with the same cleaned label map to one canonical QID\n",
    "movement_label_to_canonical = {}  # cleaned label (lowercase) -> canonical QID\n",
    "movement_qid_to_canonical = {}    # any movement QID -> canonical QID\n",
    "\n",
    "for qid in all_movement_qids:\n",
    "    entity = get_entity(qid)\n",
    "    if not entity:\n",
    "        continue\n",
    "    \n",
    "    # Get label\n",
    "    labels = entity.get(\"labels\", {})\n",
    "    raw_label = \"\"\n",
    "    if \"en\" in labels:\n",
    "        raw_label = labels[\"en\"].get(\"value\", \"\")\n",
    "    elif labels:\n",
    "        raw_label = list(labels.values())[0].get(\"value\", \"\")\n",
    "    \n",
    "    if not raw_label:\n",
    "        movement_qid_to_canonical[qid] = qid  # keep as-is\n",
    "        continue\n",
    "    \n",
    "    cleaned = clean_movement_label(raw_label)\n",
    "    cleaned_lower = cleaned.lower()\n",
    "    \n",
    "    if cleaned_lower in movement_label_to_canonical:\n",
    "        # Map to existing canonical QID\n",
    "        canonical_qid = movement_label_to_canonical[cleaned_lower]\n",
    "        movement_qid_to_canonical[qid] = canonical_qid\n",
    "        print(f\"{ts()} | Harmonizing: {qid} ({raw_label}) -> {canonical_qid} ({cleaned})\")\n",
    "    else:\n",
    "        # This becomes the canonical QID for this label\n",
    "        movement_label_to_canonical[cleaned_lower] = qid\n",
    "        movement_qid_to_canonical[qid] = qid\n",
    "\n",
    "print(f\"{ts()} | Movement QIDs: {len(all_movement_qids):,}\")\n",
    "print(f\"{ts()} | Unique labels after harmonization: {len(movement_label_to_canonical):,}\")\n",
    "\n",
    "def harmonize_movement_qids(qids):\n",
    "    \"\"\"Map a list of movement QIDs to their canonical forms, removing duplicates.\"\"\"\n",
    "    canonical = set()\n",
    "    for qid in qids:\n",
    "        canonical.add(movement_qid_to_canonical.get(qid, qid))\n",
    "    return list(canonical)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Build records from painting_qids (on demand loading)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\n{ts()} | Building records from {len(painting_qids):,} paintings...\")\n",
    "\n",
    "# Check for duplicates in painting_qids\n",
    "unique_qids = set(painting_qids)\n",
    "if len(unique_qids) != len(painting_qids):\n",
    "    print(f\"{ts()} | WARNING: {len(painting_qids) - len(unique_qids):,} duplicate Q-IDs in painting_qids\")\n",
    "\n",
    "records = {}\n",
    "linked_qids = set()\n",
    "artist_qids = set()\n",
    "start_time = time.time()\n",
    "total = len(painting_qids)\n",
    "\n",
    "# Counters for transparency\n",
    "skipped_no_entity = 0\n",
    "skipped_no_id = 0\n",
    "skipped_duplicate_uri = 0\n",
    "records_no_year = 0\n",
    "records_no_keywords = 0\n",
    "processed = 0\n",
    "\n",
    "for i, qid in enumerate(painting_qids):\n",
    "    if (i + 1) % 2000 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = (i + 1) / elapsed if elapsed > 0 else 0\n",
    "        remaining = (total - (i + 1)) / rate if rate > 0 else 0\n",
    "        print(f\"{ts()} | [building] {i + 1:,}/{total:,} @ {rate:.0f}/s | records: {len(records):,} | ~{format_time_remaining(remaining)} remaining\")\n",
    "    \n",
    "    entity = get_entity(qid)\n",
    "    \n",
    "    if entity is None:\n",
    "        skipped_no_entity += 1\n",
    "        continue\n",
    "    \n",
    "    if \"id\" not in entity:\n",
    "        skipped_no_id += 1\n",
    "        continue\n",
    "    \n",
    "    entity_id = entity[\"id\"]\n",
    "    if entity_id != qid:\n",
    "        print(f\"{ts()} | WARNING: Requested {qid} but got entity {entity_id}\")\n",
    "    \n",
    "    uri = f\"https://www.wikidata.org/wiki/{qid}\"\n",
    "    \n",
    "    if uri in records:\n",
    "        skipped_duplicate_uri += 1\n",
    "        continue\n",
    "\n",
    "    # Get title with fallback\n",
    "    title = \"\"\n",
    "    labels = entity.get(\"labels\", {})\n",
    "    if \"en\" in labels:\n",
    "        title = labels[\"en\"].get(\"value\", \"\")\n",
    "    elif labels:\n",
    "        title = list(labels.values())[0].get(\"value\", \"\")\n",
    "\n",
    "    rec = {\n",
    "        \"id\": qid,\n",
    "        \"keywords\": set(),\n",
    "        \"year\": \"\",\n",
    "        \"_title\": title,\n",
    "        \"_creation\": \"\",\n",
    "        \"_artist\": \"\",\n",
    "        \"_country\": \"\",\n",
    "        \"_depicts\": \"\",\n",
    "        \"_genre\": \"\",\n",
    "        \"_material\": \"\",\n",
    "        \"_movement\": \"\",\n",
    "        \"_museum\": \"\"\n",
    "    }\n",
    "\n",
    "    # Populate claims and collect linked Q-IDs\n",
    "    for prop, field, kw_prefix in [\n",
    "        (\"P170\", \"_artist\", \"artist\"),\n",
    "        (\"P195\", \"_museum\", \"museum\"),\n",
    "        (\"P180\", \"_depicts\", \"depicts\"),\n",
    "        (\"P186\", \"_material\", \"material\"),\n",
    "        (\"P136\", \"_genre\", \"genre\"),\n",
    "        (\"P135\", \"_movement\", \"movement\"),\n",
    "    ]:\n",
    "        qids_list = resolve_claim_qids(entity, prop)\n",
    "        \n",
    "        # Harmonize movement QIDs\n",
    "        if prop == \"P135\":\n",
    "            qids_list = harmonize_movement_qids(qids_list)\n",
    "        \n",
    "        linked_qids.update(qids_list)\n",
    "        \n",
    "        if prop == \"P170\":\n",
    "            artist_qids.update(qids_list)\n",
    "        \n",
    "        if field in [\"_depicts\", \"_material\", \"_genre\", \"_movement\"]:\n",
    "            add_column_value(rec, field, qids_list)\n",
    "        else:\n",
    "            rec[field] = \", \".join(qids_list)\n",
    "        \n",
    "        for q in qids_list:\n",
    "            add_keyword(rec, kw_prefix, q)\n",
    "\n",
    "    # Creation dates\n",
    "    years = [parse_wikidata_time(t) for t in resolve_claim_values(entity, \"P571\") if parse_wikidata_time(t)]\n",
    "    if years:\n",
    "        years = sorted(set(years))\n",
    "        if len(years) > 1:\n",
    "            rec[\"_creation\"] = f\"{years[0]}-{years[-1]}\"\n",
    "            decade_base = years[0]\n",
    "        else:\n",
    "            rec[\"_creation\"] = str(years[0])\n",
    "            decade_base = years[0]\n",
    "        rec[\"year\"] = str((decade_base // 10) * 10)\n",
    "    else:\n",
    "        records_no_year += 1\n",
    "    \n",
    "    if not rec[\"keywords\"]:\n",
    "        records_no_keywords += 1\n",
    "\n",
    "    records[uri] = rec\n",
    "    processed += 1\n",
    "\n",
    "elapsed_total = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{ts()} | [done] Building complete in {format_time_remaining(elapsed_total)}\")\n",
    "print(f\"{ts()} | \")\n",
    "print(f\"{ts()} | Input:     {len(painting_qids):,} painting Q-IDs\")\n",
    "print(f\"{ts()} | Unique:    {len(unique_qids):,} unique Q-IDs\")\n",
    "print(f\"{ts()} | Processed: {processed:,} entities\")\n",
    "print(f\"{ts()} | Output:    {len(records):,} records built\")\n",
    "print(f\"{ts()} | \")\n",
    "print(f\"{ts()} | Skipped:\")\n",
    "print(f\"{ts()} |   - No entity in cache:  {skipped_no_entity:,}\")\n",
    "print(f\"{ts()} |   - No 'id' in entity:   {skipped_no_id:,}\")\n",
    "print(f\"{ts()} |   - Duplicate URI:       {skipped_duplicate_uri:,}\")\n",
    "print(f\"{ts()} | \")\n",
    "print(f\"{ts()} | Unaccounted: {len(painting_qids) - skipped_no_entity - skipped_no_id - skipped_duplicate_uri - len(records):,}\")\n",
    "print(f\"{ts()} | \")\n",
    "print(f\"{ts()} | Records with issues:\")\n",
    "print(f\"{ts()} |   - No year:     {records_no_year:,}\")\n",
    "print(f\"{ts()} |   - No keywords: {records_no_keywords:,}\")\n",
    "print(f\"{ts()} | \")\n",
    "print(f\"{ts()} | Linked entities found: {len(linked_qids):,}\")\n",
    "print(f\"{ts()} | Artists found: {len(artist_qids):,}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Pre-fetch linked entities (for label resolution and artist data)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\n{ts()} | Checking linked entities...\")\n",
    "\n",
    "to_fetch = [qid for qid in linked_qids if not os.path.exists(os.path.join(CACHE_DIR, f\"{qid}.json\"))]\n",
    "\n",
    "print(f\"{ts()} | Linked entities: {len(linked_qids):,}\")\n",
    "print(f\"{ts()} | Already cached:  {len(linked_qids) - len(to_fetch):,}\")\n",
    "print(f\"{ts()} | To fetch:        {len(to_fetch):,}\")\n",
    "\n",
    "if to_fetch:\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\"User-Agent\": USER_AGENT})\n",
    "    \n",
    "    start_time = time.time()\n",
    "    fetch_errors = 0\n",
    "    \n",
    "    for i, qid in enumerate(to_fetch):\n",
    "        if (i + 1) % 50 == 0 or i == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = (i + 1) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (len(to_fetch) - (i + 1)) / rate if rate > 0 else 0\n",
    "            print(f\"{ts()} | [fetch] {i + 1:,}/{len(to_fetch):,} @ {rate:.1f}/s | ~{format_time_remaining(remaining)} remaining\")\n",
    "        \n",
    "        try:\n",
    "            get_entity(qid, session)\n",
    "        except Exception as e:\n",
    "            fetch_errors += 1\n",
    "            print(f\"{ts()} | {qid} | ERROR ({str(e)[:80]})\")\n",
    "    \n",
    "    elapsed_total = time.time() - start_time\n",
    "    print(f\"\\n{ts()} | [done] Fetched {len(to_fetch):,} linked entities in {format_time_remaining(elapsed_total)}\")\n",
    "    if fetch_errors:\n",
    "        print(f\"{ts()} | Fetch errors: {fetch_errors:,}\")\n",
    "else:\n",
    "    print(f\"{ts()} | [done] All linked entities already cached\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Build artist data lookup (movements + birthplace → country)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\n{ts()} | Building artist data (movements + countries)...\")\n",
    "\n",
    "artist_movements = {}\n",
    "artist_countries = {}\n",
    "birthplace_qids = set()\n",
    "\n",
    "# Also scan artist movements and add to harmonization map\n",
    "artist_movement_qids = set()\n",
    "\n",
    "for artist_qid in artist_qids:\n",
    "    artist_entity = get_entity(artist_qid)\n",
    "    if not artist_entity:\n",
    "        continue\n",
    "    \n",
    "    # Get movements (P135)\n",
    "    movements = resolve_claim_qids(artist_entity, \"P135\")\n",
    "    if movements:\n",
    "        artist_movement_qids.update(movements)\n",
    "    \n",
    "    # Get birthplace (P19)\n",
    "    birthplaces = resolve_claim_qids(artist_entity, \"P19\")\n",
    "    if birthplaces:\n",
    "        birthplace_qids.update(birthplaces)\n",
    "\n",
    "# Fetch any new movement entities from artists\n",
    "new_movement_qids = artist_movement_qids - all_movement_qids\n",
    "if new_movement_qids:\n",
    "    print(f\"{ts()} | Found {len(new_movement_qids):,} new movement QIDs from artists\")\n",
    "    \n",
    "    movements_to_fetch = [qid for qid in new_movement_qids if not os.path.exists(os.path.join(CACHE_DIR, f\"{qid}.json\"))]\n",
    "    \n",
    "    if movements_to_fetch:\n",
    "        session = requests.Session()\n",
    "        session.headers.update({\"User-Agent\": USER_AGENT})\n",
    "        \n",
    "        for qid in movements_to_fetch:\n",
    "            try:\n",
    "                get_entity(qid, session)\n",
    "            except Exception as e:\n",
    "                print(f\"{ts()} | {qid} | ERROR ({str(e)[:80]})\")\n",
    "    \n",
    "    # Add to harmonization map\n",
    "    for qid in new_movement_qids:\n",
    "        if qid in movement_qid_to_canonical:\n",
    "            continue\n",
    "        \n",
    "        entity = get_entity(qid)\n",
    "        if not entity:\n",
    "            movement_qid_to_canonical[qid] = qid\n",
    "            continue\n",
    "        \n",
    "        labels = entity.get(\"labels\", {})\n",
    "        raw_label = \"\"\n",
    "        if \"en\" in labels:\n",
    "            raw_label = labels[\"en\"].get(\"value\", \"\")\n",
    "        elif labels:\n",
    "            raw_label = list(labels.values())[0].get(\"value\", \"\")\n",
    "        \n",
    "        if not raw_label:\n",
    "            movement_qid_to_canonical[qid] = qid\n",
    "            continue\n",
    "        \n",
    "        cleaned = clean_movement_label(raw_label)\n",
    "        cleaned_lower = cleaned.lower()\n",
    "        \n",
    "        if cleaned_lower in movement_label_to_canonical:\n",
    "            canonical_qid = movement_label_to_canonical[cleaned_lower]\n",
    "            movement_qid_to_canonical[qid] = canonical_qid\n",
    "            print(f\"{ts()} | Harmonizing (artist): {qid} ({raw_label}) -> {canonical_qid} ({cleaned})\")\n",
    "        else:\n",
    "            movement_label_to_canonical[cleaned_lower] = qid\n",
    "            movement_qid_to_canonical[qid] = qid\n",
    "\n",
    "# Now build artist_movements with harmonized QIDs\n",
    "for artist_qid in artist_qids:\n",
    "    artist_entity = get_entity(artist_qid)\n",
    "    if not artist_entity:\n",
    "        continue\n",
    "    \n",
    "    movements = resolve_claim_qids(artist_entity, \"P135\")\n",
    "    if movements:\n",
    "        harmonized = harmonize_movement_qids(movements)\n",
    "        artist_movements[artist_qid] = harmonized\n",
    "        linked_qids.update(harmonized)\n",
    "\n",
    "print(f\"{ts()} | Artists with movements: {len(artist_movements):,}\")\n",
    "print(f\"{ts()} | Birthplaces found: {len(birthplace_qids):,}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Fetch birthplace entities to get their countries\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\n{ts()} | Fetching birthplace entities...\")\n",
    "\n",
    "to_fetch_birthplaces = [qid for qid in birthplace_qids if not os.path.exists(os.path.join(CACHE_DIR, f\"{qid}.json\"))]\n",
    "\n",
    "if to_fetch_birthplaces:\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\"User-Agent\": USER_AGENT})\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, qid in enumerate(to_fetch_birthplaces):\n",
    "        if (i + 1) % 50 == 0 or i == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = (i + 1) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (len(to_fetch_birthplaces) - (i + 1)) / rate if rate > 0 else 0\n",
    "            print(f\"{ts()} | [fetch] {i + 1:,}/{len(to_fetch_birthplaces):,} @ {rate:.1f}/s | ~{format_time_remaining(remaining)} remaining\")\n",
    "        \n",
    "        try:\n",
    "            get_entity(qid, session)\n",
    "        except Exception as e:\n",
    "            print(f\"{ts()} | {qid} | ERROR ({str(e)[:80]})\")\n",
    "    \n",
    "    print(f\"{ts()} | [done] Fetched {len(to_fetch_birthplaces):,} birthplace entities\")\n",
    "else:\n",
    "    print(f\"{ts()} | [done] All birthplace entities already cached\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Build birthplace → country mapping (using SPARQL for current country)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\n{ts()} | Resolving birthplace → current country...\")\n",
    "\n",
    "def get_current_country_for_place(place_qid):\n",
    "    \"\"\"\n",
    "    Get the current country for a place using SPARQL.\n",
    "    Looks for country (P17) without an end time (P582).\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT ?country WHERE {{\n",
    "      wd:{place_qid} p:P17 ?stmt .\n",
    "      ?stmt ps:P17 ?country .\n",
    "      FILTER NOT EXISTS {{ ?stmt pq:P582 ?endTime }}\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = get_sparql(query)\n",
    "        bindings = result.get(\"results\", {}).get(\"bindings\", [])\n",
    "        if bindings:\n",
    "            country_uri = bindings[0].get(\"country\", {}).get(\"value\", \"\")\n",
    "            if country_uri:\n",
    "                return country_uri.split(\"/\")[-1]\n",
    "    except Exception as e:\n",
    "        # Fallback: just use first country from entity\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "# First try to get current country via SPARQL for each unique birthplace\n",
    "# To avoid too many queries, batch them\n",
    "\n",
    "birthplace_to_country = {}\n",
    "country_qids = set()\n",
    "\n",
    "# Build a single SPARQL query for all birthplaces\n",
    "print(f\"{ts()} | Querying current countries for {len(birthplace_qids):,} birthplaces...\")\n",
    "\n",
    "# Split into batches to avoid query timeout\n",
    "BATCH_SIZE = 100\n",
    "birthplace_list = list(birthplace_qids)\n",
    "\n",
    "for batch_start in range(0, len(birthplace_list), BATCH_SIZE):\n",
    "    batch = birthplace_list[batch_start:batch_start + BATCH_SIZE]\n",
    "    \n",
    "    if (batch_start // BATCH_SIZE) % 10 == 0:\n",
    "        print(f\"{ts()} | [batch] {batch_start:,}/{len(birthplace_list):,}\")\n",
    "    \n",
    "    # Build VALUES clause for batch query\n",
    "    values_clause = \" \".join([f\"wd:{qid}\" for qid in batch])\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT ?place ?country WHERE {{\n",
    "      VALUES ?place {{ {values_clause} }}\n",
    "      ?place p:P17 ?stmt .\n",
    "      ?stmt ps:P17 ?country .\n",
    "      FILTER NOT EXISTS {{ ?stmt pq:P582 ?endTime }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = get_sparql(query)\n",
    "        bindings = result.get(\"results\", {}).get(\"bindings\", [])\n",
    "        \n",
    "        for b in bindings:\n",
    "            place_uri = b.get(\"place\", {}).get(\"value\", \"\")\n",
    "            country_uri = b.get(\"country\", {}).get(\"value\", \"\")\n",
    "            \n",
    "            if place_uri and country_uri:\n",
    "                place_qid = place_uri.split(\"/\")[-1]\n",
    "                country_qid = country_uri.split(\"/\")[-1]\n",
    "                \n",
    "                # Only take first result per place\n",
    "                if place_qid not in birthplace_to_country:\n",
    "                    birthplace_to_country[place_qid] = country_qid\n",
    "                    country_qids.add(country_qid)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"{ts()} | Batch query error: {str(e)[:80]}\")\n",
    "\n",
    "# For any birthplaces not resolved, fall back to cached entity data\n",
    "fallback_count = 0\n",
    "for bp_qid in birthplace_qids:\n",
    "    if bp_qid in birthplace_to_country:\n",
    "        continue\n",
    "    \n",
    "    bp_entity = get_entity(bp_qid)\n",
    "    if not bp_entity:\n",
    "        continue\n",
    "    \n",
    "    countries = resolve_claim_qids(bp_entity, \"P17\")\n",
    "    if countries:\n",
    "        birthplace_to_country[bp_qid] = countries[0]\n",
    "        country_qids.add(countries[0])\n",
    "        fallback_count += 1\n",
    "\n",
    "print(f\"{ts()} | Birthplaces with current country: {len(birthplace_to_country):,}\")\n",
    "print(f\"{ts()} | Resolved via SPARQL: {len(birthplace_to_country) - fallback_count:,}\")\n",
    "print(f\"{ts()} | Fallback to first country: {fallback_count:,}\")\n",
    "print(f\"{ts()} | Unique countries: {len(country_qids):,}\")\n",
    "\n",
    "# Add countries to linked_qids for label resolution\n",
    "linked_qids.update(country_qids)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Build artist → country mapping\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "for artist_qid in artist_qids:\n",
    "    artist_entity = get_entity(artist_qid)\n",
    "    if not artist_entity:\n",
    "        continue\n",
    "    \n",
    "    birthplaces = resolve_claim_qids(artist_entity, \"P19\")\n",
    "    if birthplaces and birthplaces[0] in birthplace_to_country:\n",
    "        artist_countries[artist_qid] = birthplace_to_country[birthplaces[0]]\n",
    "\n",
    "print(f\"{ts()} | Artists with country: {len(artist_countries):,}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Enrich paintings with artist movements and countries\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\n{ts()} | Enriching paintings with artist data...\")\n",
    "\n",
    "enriched_movements = 0\n",
    "enriched_countries = 0\n",
    "\n",
    "for uri, rec in records.items():\n",
    "    # Get artist QIDs from the record\n",
    "    artist_str = rec.get(\"_artist\", \"\")\n",
    "    if not artist_str:\n",
    "        continue\n",
    "    \n",
    "    painting_artist_qids = [q.strip() for q in artist_str.split(\",\") if q.strip().startswith(\"Q\")]\n",
    "    \n",
    "    # Enrich movements (only if painting has none)\n",
    "    if not rec[\"_movement\"]:\n",
    "        inherited_movements = set()\n",
    "        for artist_qid in painting_artist_qids:\n",
    "            if artist_qid in artist_movements:\n",
    "                inherited_movements.update(artist_movements[artist_qid])\n",
    "        \n",
    "        if inherited_movements:\n",
    "            add_column_value(rec, \"_movement\", list(inherited_movements))\n",
    "            for m in inherited_movements:\n",
    "                add_keyword(rec, \"movement\", m)\n",
    "            enriched_movements += 1\n",
    "    \n",
    "    # Enrich countries (always, from first artist with country)\n",
    "    if not rec[\"_country\"]:\n",
    "        for artist_qid in painting_artist_qids:\n",
    "            if artist_qid in artist_countries:\n",
    "                country_qid = artist_countries[artist_qid]\n",
    "                rec[\"_country\"] = country_qid\n",
    "                add_keyword(rec, \"country\", country_qid)\n",
    "                enriched_countries += 1\n",
    "                break  # take first artist's country\n",
    "\n",
    "print(f\"{ts()} | Enriched {enriched_movements:,} paintings with artist movements\")\n",
    "print(f\"{ts()} | Enriched {enriched_countries:,} paintings with artist countries\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Fetch any new linked entities (movements + countries)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "new_to_fetch = [qid for qid in linked_qids if not os.path.exists(os.path.join(CACHE_DIR, f\"{qid}.json\"))]\n",
    "\n",
    "if new_to_fetch:\n",
    "    print(f\"\\n{ts()} | Fetching {len(new_to_fetch):,} new entities (movements/countries)...\")\n",
    "    \n",
    "    session = requests.Session()\n",
    "    session.headers.update({\"User-Agent\": USER_AGENT})\n",
    "    \n",
    "    for i, qid in enumerate(new_to_fetch):\n",
    "        try:\n",
    "            get_entity(qid, session)\n",
    "        except Exception as e:\n",
    "            print(f\"{ts()} | {qid} | ERROR ({str(e)[:80]})\")\n",
    "    \n",
    "    print(f\"{ts()} | [done]\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Final stats\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "paintings_with_movement = sum(1 for rec in records.values() if rec[\"_movement\"])\n",
    "paintings_with_country = sum(1 for rec in records.values() if rec[\"_country\"])\n",
    "\n",
    "print(f\"\\n{ts()} | Final stats:\")\n",
    "print(f\"{ts()} |   - Paintings with movements: {paintings_with_movement:,} / {len(records):,} ({100*paintings_with_movement/len(records):.1f}%)\")\n",
    "print(f\"{ts()} |   - Paintings with country:   {paintings_with_country:,} / {len(records):,} ({100*paintings_with_country/len(records):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. WRITE CSV (with claim labels & logging)\n",
    "# =============================================================================\n",
    "\n",
    "os.makedirs(\"vv/data\", exist_ok=True)\n",
    "csv_path = os.path.join(\"vv/data\", \"data.csv\")\n",
    "\n",
    "# Remove existing CSV if present\n",
    "if os.path.exists(csv_path):\n",
    "    os.remove(csv_path)\n",
    "    print(f\"{ts()} | Removed existing {csv_path}\")\n",
    "\n",
    "FIELDNAMES = [\n",
    "    \"id\",\n",
    "    \"keywords\",\n",
    "    \"year\",\n",
    "    \"_title\",\n",
    "    \"_creation\",\n",
    "    \"_artist\",\n",
    "    \"_country\",\n",
    "    \"_depicts\",\n",
    "    \"_genre\",\n",
    "    \"_material\",\n",
    "    \"_movement\",\n",
    "    \"_museum\",\n",
    "]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Helper function for cleaning movement labels (if not already defined)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def clean_movement_label(label):\n",
    "    \"\"\"Clean up movement labels by removing ' painting' suffix.\"\"\"\n",
    "    if not label:\n",
    "        return label\n",
    "    if label.lower().endswith(\" painting\"):\n",
    "        label = label[:-9]\n",
    "    return label.title()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Pre-load all linked entity labels into memory\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"{ts()} | Pre-loading labels for {len(linked_qids):,} linked entities...\")\n",
    "\n",
    "labels_cache = {}\n",
    "missing_labels = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for i, qid in enumerate(linked_qids):\n",
    "    if (i + 1) % 5000 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = (i + 1) / elapsed if elapsed > 0 else 0\n",
    "        remaining = (len(linked_qids) - (i + 1)) / rate if rate > 0 else 0\n",
    "        print(f\"{ts()} | [loading labels] {i + 1:,}/{len(linked_qids):,} @ {rate:.0f}/s | ~{format_time_remaining(remaining)} remaining\")\n",
    "    \n",
    "    cache_file = os.path.join(CACHE_DIR, f\"{qid}.json\")\n",
    "    if os.path.exists(cache_file):\n",
    "        try:\n",
    "            with open(cache_file, \"r\") as f:\n",
    "                entity = json.load(f)\n",
    "            \n",
    "            labels = entity.get(\"labels\", {})\n",
    "            label = None\n",
    "            if \"en\" in labels:\n",
    "                label = labels[\"en\"].get(\"value\")\n",
    "            elif labels:\n",
    "                label = list(labels.values())[0].get(\"value\")\n",
    "            \n",
    "            if label:\n",
    "                labels_cache[qid] = label\n",
    "            else:\n",
    "                labels_cache[qid] = qid\n",
    "                missing_labels += 1\n",
    "        except:\n",
    "            labels_cache[qid] = qid\n",
    "            missing_labels += 1\n",
    "    else:\n",
    "        labels_cache[qid] = qid\n",
    "        missing_labels += 1\n",
    "\n",
    "elapsed_total = time.time() - start_time\n",
    "print(f\"{ts()} | [done] Loaded {len(labels_cache):,} labels ({missing_labels:,} missing, using Q-ID) in {format_time_remaining(elapsed_total)}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Helper functions\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def get_label_fast(qid):\n",
    "    \"\"\"Get label from in-memory cache.\"\"\"\n",
    "    return labels_cache.get(qid, qid)\n",
    "\n",
    "def normalize_label(label):\n",
    "    \"\"\"Normalize label to title case for consistency.\"\"\"\n",
    "    if not label:\n",
    "        return label\n",
    "    # Title case, but preserve all-caps acronyms\n",
    "    return label[0].upper() + label[1:] if len(label) > 1 else label.upper()\n",
    "\n",
    "def dedupe_labels_case_insensitive(labels_list):\n",
    "    \"\"\"\n",
    "    Deduplicate labels that differ only by capitalization.\n",
    "    Keeps the first occurrence's capitalization, normalized to title case.\n",
    "    \"\"\"\n",
    "    seen = {}  # lowercase -> normalized label\n",
    "    result = []\n",
    "    \n",
    "    for label in labels_list:\n",
    "        if not label:\n",
    "            continue\n",
    "        lower = label.lower()\n",
    "        if lower not in seen:\n",
    "            # Normalize: capitalize first letter\n",
    "            normalized = normalize_label(label)\n",
    "            seen[lower] = normalized\n",
    "            result.append(normalized)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def resolve_keyword(kw):\n",
    "    \"\"\"Resolve keyword like 'artist:Q5592' to 'artist:Rogier van der Weyden'.\"\"\"\n",
    "    if not kw or \":\" not in kw:\n",
    "        return None\n",
    "    \n",
    "    prefix, value = kw.split(\":\", 1)\n",
    "    \n",
    "    if not prefix or not value:\n",
    "        return None\n",
    "    \n",
    "    if value.startswith(\"Q\"):\n",
    "        label = get_label_fast(value)\n",
    "        if label:\n",
    "            label = normalize_value(label)\n",
    "            label = normalize_label(label)\n",
    "            # Clean movement labels\n",
    "            if prefix == \"movement\":\n",
    "                label = clean_movement_label(label)\n",
    "        if not label:\n",
    "            label = value\n",
    "        return f\"{prefix}:{label}\"\n",
    "    \n",
    "    return kw\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Check current state\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\n{ts()} | Checking current state...\")\n",
    "\n",
    "painting_qids_set = set(painting_qids)\n",
    "image_files = [f for f in os.listdir(IMAGES_DIR) if f.lower().endswith(\".jpg\")]\n",
    "image_qids = set(os.path.splitext(f)[0] for f in image_files)\n",
    "\n",
    "print(f\"{ts()} | Paintings in query: {len(painting_qids_set):,}\")\n",
    "print(f\"{ts()} | Images in folder: {len(image_qids):,}\")\n",
    "print(f\"{ts()} | Images matching query: {len(image_qids & painting_qids_set):,}\")\n",
    "print(f\"{ts()} | Images NOT in query: {len(image_qids - painting_qids_set):,}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Filter records that have image and year\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\n{ts()} | Filtering records with image and year...\")\n",
    "\n",
    "eligible_records = []\n",
    "no_record = 0\n",
    "no_image = 0\n",
    "no_year = 0\n",
    "\n",
    "for qid in painting_qids:\n",
    "    uri = f\"https://www.wikidata.org/wiki/{qid}\"\n",
    "    \n",
    "    if uri not in records:\n",
    "        no_record += 1\n",
    "        continue\n",
    "    \n",
    "    rec = records[uri]\n",
    "    img_path = os.path.join(IMAGES_DIR, f\"{qid}.jpg\")\n",
    "    year = rec.get(\"year\")\n",
    "    \n",
    "    if not os.path.exists(img_path):\n",
    "        no_image += 1\n",
    "        continue\n",
    "    \n",
    "    if year is None or year == \"\":\n",
    "        no_year += 1\n",
    "        continue\n",
    "    \n",
    "    eligible_records.append(rec)\n",
    "\n",
    "print(f\"{ts()} | Found {len(eligible_records):,} eligible records\")\n",
    "print(f\"{ts()} | Skipped: {no_record:,} no record, {no_image:,} no image, {no_year:,} no year\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Resolve labels for keywords and claim fields\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\n{ts()} | Resolving labels...\")\n",
    "\n",
    "total = len(eligible_records)\n",
    "start_time = time.time()\n",
    "\n",
    "for i, rec in enumerate(eligible_records):\n",
    "    if (i + 1) % 2000 == 0 or i == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = (i + 1) / elapsed if elapsed > 0 else 0\n",
    "        remaining = (total - (i + 1)) / rate if rate > 0 else 0\n",
    "        print(f\"{ts()} | [resolving] {i + 1:,}/{total:,} @ {rate:.0f}/s | ~{format_time_remaining(remaining)} remaining\")\n",
    "    \n",
    "    # Convert keywords set to sorted string with resolved labels\n",
    "    resolved = []\n",
    "    for kw in rec[\"keywords\"]:\n",
    "        r = resolve_keyword(kw)\n",
    "        if r and not r.endswith(\":\"):\n",
    "            resolved.append(r)\n",
    "    \n",
    "    # Deduplicate keywords by case-insensitive comparison\n",
    "    seen_keywords = {}\n",
    "    unique_keywords = []\n",
    "    for kw in resolved:\n",
    "        lower = kw.lower()\n",
    "        if lower not in seen_keywords:\n",
    "            seen_keywords[lower] = kw\n",
    "            unique_keywords.append(kw)\n",
    "    \n",
    "    rec[\"keywords\"] = \",\".join(sorted(unique_keywords)) if unique_keywords else \"\"\n",
    "\n",
    "    # Replace QIDs with labels in claim fields (with case-insensitive deduplication)\n",
    "    for field in [\"_artist\", \"_depicts\", \"_material\", \"_genre\", \"_movement\", \"_museum\", \"_country\"]:\n",
    "        qids_str = rec.get(field, \"\")\n",
    "        if qids_str:\n",
    "            labels = []\n",
    "            for q in qids_str.split(\",\"):\n",
    "                q = q.strip()\n",
    "                if q.startswith(\"Q\"):\n",
    "                    label = get_label_fast(q)\n",
    "                    if label:\n",
    "                        label = normalize_value(label)\n",
    "                        # Clean movement labels\n",
    "                        if field == \"_movement\":\n",
    "                            label = clean_movement_label(label)\n",
    "                    if not label:\n",
    "                        label = q\n",
    "                    labels.append(label)\n",
    "                elif q:\n",
    "                    labels.append(q)\n",
    "            \n",
    "            # Deduplicate labels that differ only by capitalization\n",
    "            deduped = dedupe_labels_case_insensitive(labels)\n",
    "            rec[field] = \", \".join(deduped)\n",
    "\n",
    "elapsed_total = time.time() - start_time\n",
    "print(f\"{ts()} | [done] Resolved labels in {format_time_remaining(elapsed_total)}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Write CSV\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\n{ts()} | Writing CSV...\")\n",
    "\n",
    "written = 0\n",
    "written_qids = set()\n",
    "skipped_no_keywords = 0\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    writer = csv.DictWriter(fh, fieldnames=FIELDNAMES, quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for rec in eligible_records:\n",
    "        qid = rec[\"id\"]\n",
    "        keywords = rec.get(\"keywords\", \"\")\n",
    "        \n",
    "        if not keywords:\n",
    "            skipped_no_keywords += 1\n",
    "            continue\n",
    "        \n",
    "        writer.writerow({fn: rec.get(fn, \"\") for fn in FIELDNAMES})\n",
    "        written += 1\n",
    "        written_qids.add(qid)\n",
    "\n",
    "csv_size = os.path.getsize(csv_path)\n",
    "print(f\"{ts()} | CSV written: {written:,} artworks, {csv_size / 1024 / 1024:.1f} MB → {csv_path}\")\n",
    "print(f\"{ts()} | Skipped {skipped_no_keywords:,} records with no keywords\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Move images NOT in painting_qids OR not written to CSV\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\n{ts()} | Cleaning up images...\")\n",
    "\n",
    "os.makedirs(SKIPPED_DIR, exist_ok=True)\n",
    "\n",
    "moved = 0\n",
    "kept = 0\n",
    "\n",
    "for fname in image_files:\n",
    "    qid = os.path.splitext(fname)[0]\n",
    "    src = os.path.join(IMAGES_DIR, fname)\n",
    "    \n",
    "    if qid in painting_qids_set and qid in written_qids:\n",
    "        kept += 1\n",
    "        continue\n",
    "    \n",
    "    dst = os.path.join(SKIPPED_DIR, fname)\n",
    "    \n",
    "    if os.path.exists(dst):\n",
    "        base, ext = os.path.splitext(fname)\n",
    "        dst = os.path.join(SKIPPED_DIR, f\"{base}__dup{ext}\")\n",
    "    \n",
    "    shutil.move(src, dst)\n",
    "    moved += 1\n",
    "\n",
    "print(f\"{ts()} | Kept {kept:,} images, moved {moved:,} to {SKIPPED_DIR}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Summary\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Paintings in query:      {len(painting_qids):,}\")\n",
    "print(f\"Records built:           {len(records):,}\")\n",
    "print(f\"Eligible (image+year):   {len(eligible_records):,}\")\n",
    "print(f\"Skipped (no keywords):   {skipped_no_keywords:,}\")\n",
    "print(f\"Written to CSV:          {written:,}\")\n",
    "print(f\"CSV size:                {csv_size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"Images kept:             {kept:,}\")\n",
    "print(f\"Images moved to skipped: {moved:,}\")\n",
    "print(f\"Output: {csv_path}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. RUN vikus-viewer-script to create textures and spritesheet assets\n",
    "# see: https://github.com/cpietsch/vikus-viewer-script\n",
    "\n",
    "# !vikus-viewer-script \"./data/images/*.jpg\"\n",
    "\n",
    "!cd vv; vikus-viewer-script \"./data/images/*.jpg\" --textureFormat avif --textureQuality 50 --spriteFormat avif --spriteQuality 50 --spriteSize 96 --mediumSize 1024 --largeSize 4096"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "5e3dd10ae0e545ba9f11af37ab1fbf0d",
  "deepnote_persisted_session": {
   "createdAt": "2025-04-16T14:13:27.506Z"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
